{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T03:20:11.302341Z",
          "iopub.status.busy": "2023-01-22T03:20:11.302015Z",
          "iopub.status.idle": "2023-01-22T03:20:11.309379Z",
          "shell.execute_reply": "2023-01-22T03:20:11.308575Z",
          "shell.execute_reply.started": "2023-01-22T03:20:11.302280Z"
        },
        "id": "h5hjCcLDr2WC"
      },
      "outputs": [],
      "source": [
        "# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:21:30.664394Z",
          "iopub.status.busy": "2023-01-22T09:21:30.663844Z",
          "iopub.status.idle": "2023-01-22T09:22:51.240473Z",
          "shell.execute_reply": "2023-01-22T09:22:51.239834Z",
          "shell.execute_reply.started": "2023-01-22T09:21:30.664329Z"
        },
        "id": "weQHKYPZ-C1y",
        "outputId": "20478c60-162a-456a-e70f-3ce7c411fc55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu117\n",
            "Collecting numpy\n",
            "  Downloading https://download.pytorch.org/whl/nightly/numpy-1.24.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu117/torch-2.0.0.dev20230121%2Bcu117-cp39-cp39-linux_x86_64.whl (1821.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading https://download.pytorch.org/whl/nightly/sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading https://download.pytorch.org/whl/nightly/typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting pytorch-triton==2.0.0+0d7e753227\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-2.0.0%2B0d7e753227-cp39-cp39-linux_x86_64.whl (18.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading https://download.pytorch.org/whl/nightly/networkx-3.0rc1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading https://download.pytorch.org/whl/nightly/filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting cmake\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting mpmath>=0.19\n",
            "  Downloading https://download.pytorch.org/whl/nightly/mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, cmake, typing-extensions, sympy, numpy, networkx, filelock, pytorch-triton, torch\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.3.0\n",
            "    Uninstalling typing_extensions-4.3.0:\n",
            "      Successfully uninstalled typing_extensions-4.3.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.1\n",
            "    Uninstalling numpy-1.23.1:\n",
            "      Successfully uninstalled numpy-1.23.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 2.8.4\n",
            "    Uninstalling networkx-2.8.4:\n",
            "      Successfully uninstalled networkx-2.8.4\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.7.1\n",
            "    Uninstalling filelock-3.7.1:\n",
            "      Successfully uninstalled filelock-3.7.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu116\n",
            "    Uninstalling torch-1.12.0+cu116:\n",
            "      Successfully uninstalled torch-1.12.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu116 requires torch==1.12.0, but you have torch 2.0.0.dev20230121+cu117 which is incompatible.\n",
            "torchaudio 0.12.0+cu116 requires torch==1.12.0, but you have torch 2.0.0.dev20230121+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cmake-3.25.0 filelock-3.9.0 mpmath-1.2.1 networkx-3.0rc1 numpy-1.24.1 pytorch-triton-2.0.0+0d7e753227 sympy-1.11.1 torch-2.0.0.dev20230121+cu117 typing-extensions-4.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T03:22:12.866322Z",
          "iopub.status.busy": "2023-01-22T03:22:12.866128Z",
          "iopub.status.idle": "2023-01-22T03:22:15.955696Z",
          "shell.execute_reply": "2023-01-22T03:22:15.954883Z",
          "shell.execute_reply.started": "2023-01-22T03:22:12.866301Z"
        },
        "id": "JZbpDx7w-C1y",
        "outputId": "933803e0-b0ba-405b-9523-488903398d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "backprop.txt   gpt_dev.ipynb  micrograd.txt  story.txt\n",
            "batchnorm.txt  input.txt      mlp.txt\t     wavenet.txt\n",
            "gpt.txt        makemore.txt   pytorch\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:22:51.241796Z",
          "iopub.status.busy": "2023-01-22T09:22:51.241588Z",
          "iopub.status.idle": "2023-01-22T09:22:52.448300Z",
          "shell.execute_reply": "2023-01-22T09:22:52.447475Z",
          "shell.execute_reply.started": "2023-01-22T09:22:51.241773Z"
        },
        "id": "sII14x7D-C1z",
        "outputId": "393cd45c-608a-486d-f664-7cb79687c6dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.0.dev20230121+cu117\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:04.679948Z",
          "iopub.status.busy": "2023-01-22T09:24:04.679369Z",
          "iopub.status.idle": "2023-01-22T09:24:04.694371Z",
          "shell.execute_reply": "2023-01-22T09:24:04.693794Z",
          "shell.execute_reply.started": "2023-01-22T09:24:04.679923Z"
        },
        "id": "KV_KtTXB-C1z",
        "outputId": "38dcbb18-41d6-408f-8d9f-c00f59d67ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:05.104442Z",
          "iopub.status.busy": "2023-01-22T09:24:05.103901Z",
          "iopub.status.idle": "2023-01-22T09:24:05.120194Z",
          "shell.execute_reply": "2023-01-22T09:24:05.119556Z",
          "shell.execute_reply.started": "2023-01-22T09:24:05.104417Z"
        },
        "id": "cmjtKwxv-C1z"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:05.372732Z",
          "iopub.status.busy": "2023-01-22T09:24:05.372185Z",
          "iopub.status.idle": "2023-01-22T09:24:05.375843Z",
          "shell.execute_reply": "2023-01-22T09:24:05.375278Z",
          "shell.execute_reply.started": "2023-01-22T09:24:05.372706Z"
        },
        "id": "ohre0Laf-C1z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:05.637272Z",
          "iopub.status.busy": "2023-01-22T09:24:05.636598Z",
          "iopub.status.idle": "2023-01-22T09:24:05.744800Z",
          "shell.execute_reply": "2023-01-22T09:24:05.744165Z",
          "shell.execute_reply.started": "2023-01-22T09:24:05.637245Z"
        },
        "id": "5HdnOKPV-C10"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data)- block_size, (batch_size,))\n",
        "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4ack0w--C10"
      },
      "source": [
        "Below this line, I will run the code below again for building another model.\n",
        "That is because the code above using text. If I load other text file, it will not be text varible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:06.102480Z",
          "iopub.status.busy": "2023-01-22T09:24:06.101951Z",
          "iopub.status.idle": "2023-01-22T09:24:06.106353Z",
          "shell.execute_reply": "2023-01-22T09:24:06.105800Z",
          "shell.execute_reply.started": "2023-01-22T09:24:06.102457Z"
        },
        "id": "pXDjUTZs-C10"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:06.337046Z",
          "iopub.status.busy": "2023-01-22T09:24:06.336553Z",
          "iopub.status.idle": "2023-01-22T09:24:06.340458Z",
          "shell.execute_reply": "2023-01-22T09:24:06.339887Z",
          "shell.execute_reply.started": "2023-01-22T09:24:06.337023Z"
        },
        "id": "7H4KE6Ib-C11"
      },
      "outputs": [],
      "source": [
        "def new_gelu(x):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChYiIL1A-C11"
      },
      "source": [
        "- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:06.801183Z",
          "iopub.status.busy": "2023-01-22T09:24:06.800651Z",
          "iopub.status.idle": "2023-01-22T09:24:06.806408Z",
          "shell.execute_reply": "2023-01-22T09:24:06.805875Z",
          "shell.execute_reply.started": "2023-01-22T09:24:06.801158Z"
        },
        "id": "-TmVPV41-C11",
        "outputId": "368dfa2b-052b-4976-eab6-20a63f1d8a9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.zeros((4, 2,8,8)) [0,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:07.053375Z",
          "iopub.status.busy": "2023-01-22T09:24:07.052851Z",
          "iopub.status.idle": "2023-01-22T09:24:07.059672Z",
          "shell.execute_reply": "2023-01-22T09:24:07.059075Z",
          "shell.execute_reply.started": "2023-01-22T09:24:07.053351Z"
        },
        "id": "R16owyfz-C11"
      },
      "outputs": [],
      "source": [
        "tril1 = torch.tril(torch.ones(8, 8))\n",
        "tril2 = torch.tril(torch.ones(8, 8)).view(1, 1, 8, 8)\n",
        "o1 = torch.randn((4, 2, 8, 8))\n",
        "m1 = o1.masked_fill(tril1 == 0, float('-inf'))\n",
        "m2 = o1.masked_fill(tril2 == 0, float('-inf'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:07.299139Z",
          "iopub.status.busy": "2023-01-22T09:24:07.298618Z",
          "iopub.status.idle": "2023-01-22T09:24:07.303265Z",
          "shell.execute_reply": "2023-01-22T09:24:07.302642Z",
          "shell.execute_reply.started": "2023-01-22T09:24:07.299115Z"
        },
        "id": "rkm5Yfx_-C12",
        "outputId": "79208bfc-a265-41ff-b284-0b1d13996f0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.allclose(m1, m2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:07.541786Z",
          "iopub.status.busy": "2023-01-22T09:24:07.541174Z",
          "iopub.status.idle": "2023-01-22T09:24:07.544788Z",
          "shell.execute_reply": "2023-01-22T09:24:07.544154Z",
          "shell.execute_reply.started": "2023-01-22T09:24:07.541757Z"
        },
        "id": "qbURyRC9-C12"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia\n",
        "# print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i87xgunB-C12"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:33:45.396679Z",
          "iopub.status.busy": "2023-01-22T09:33:45.396029Z",
          "iopub.status.idle": "2023-01-22T09:33:45.403133Z",
          "shell.execute_reply": "2023-01-22T09:33:45.402541Z",
          "shell.execute_reply.started": "2023-01-22T09:33:45.396654Z"
        },
        "id": "UPhj0H-0-C12"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.qkv = nn.Linear(n_embd, n_embd*3, bias=False)\n",
        "        \n",
        "        #building a mask for matrix size of  # (B, T, nh, nh)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.n_head = num_heads\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        q, k, v = self.qkv(x).split(n_embd, dim=2) # (B,T,C*3) to 3 of  (B,T,C)  \n",
        "\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2) #  (B,T, nh, hs)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2) #  (B,T, nh, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2) #  (B,T, nh, hs)\n",
        "        \n",
        "        # (B, T, nh, hs) @ (B, T, hs, nh) -> (B, T, nh, nh)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 \n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, nh, nh)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        \n",
        "        out = wei @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "            \n",
        "\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:33:45.762732Z",
          "iopub.status.busy": "2023-01-22T09:33:45.762445Z",
          "iopub.status.idle": "2023-01-22T09:33:45.767452Z",
          "shell.execute_reply": "2023-01-22T09:33:45.766876Z",
          "shell.execute_reply.started": "2023-01-22T09:33:45.762709Z"
        },
        "id": "N6JP1yl6-C13"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        #just a comment\n",
        "        # self.net = nn.Sequential(\n",
        "        #     nn.Linear(n_embd, 4 * n_embd),\n",
        "        #     new_gelu,\n",
        "        #     nn.Linear(4 * n_embd, n_embd),\n",
        "        #     nn.Dropout(dropout),\n",
        "        # )\n",
        "        self.feature_extract = nn.Linear(n_embd, 4 * n_embd)\n",
        "        self.feature_detract = nn.Linear(4 * n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.feature_extract(x)\n",
        "        x = new_gelu(x)\n",
        "        x = self.feature_detract(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:33:46.296754Z",
          "iopub.status.busy": "2023-01-22T09:33:46.296467Z",
          "iopub.status.idle": "2023-01-22T09:33:46.301327Z",
          "shell.execute_reply": "2023-01-22T09:33:46.300773Z",
          "shell.execute_reply.started": "2023-01-22T09:33:46.296730Z"
        },
        "id": "Z5YZdABY-C13"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:33:49.392418Z",
          "iopub.status.busy": "2023-01-22T09:33:49.391747Z",
          "iopub.status.idle": "2023-01-22T09:33:49.399414Z",
          "shell.execute_reply": "2023-01-22T09:33:49.398831Z",
          "shell.execute_reply.started": "2023-01-22T09:33:49.392393Z"
        },
        "id": "RlON3YA0-C13"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:28:35.653867Z",
          "iopub.status.busy": "2023-01-22T09:28:35.653317Z",
          "iopub.status.idle": "2023-01-22T09:28:35.659582Z",
          "shell.execute_reply": "2023-01-22T09:28:35.658813Z",
          "shell.execute_reply.started": "2023-01-22T09:28:35.653844Z"
        },
        "id": "glAOFrWV-C14",
        "outputId": "e9257a9c-16ca-492c-bbd8-9690676d542a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbe682673d0>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 512\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:35:27.615674Z",
          "iopub.status.busy": "2023-01-22T09:35:27.615100Z",
          "iopub.status.idle": "2023-01-22T09:35:27.618619Z",
          "shell.execute_reply": "2023-01-22T09:35:27.618111Z",
          "shell.execute_reply.started": "2023-01-22T09:35:27.615649Z"
        },
        "id": "c8hwX3nv-C14"
      },
      "outputs": [],
      "source": [
        "model = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:35:58.616488Z",
          "iopub.status.busy": "2023-01-22T09:35:58.615683Z",
          "iopub.status.idle": "2023-01-22T09:35:58.730183Z",
          "shell.execute_reply": "2023-01-22T09:35:58.729486Z",
          "shell.execute_reply.started": "2023-01-22T09:35:58.616464Z"
        },
        "id": "b1LzDUyb-C14",
        "outputId": "b48152b9-bfa9-456a-8dd5-e7c916e6f9b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "19.198109 M parameters\n"
          ]
        }
      ],
      "source": [
        "print(device)\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "# m = torch.compile(mod01)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:24:12.124265Z",
          "iopub.status.busy": "2023-01-22T09:24:12.123973Z",
          "iopub.status.idle": "2023-01-22T09:24:12.127830Z",
          "shell.execute_reply": "2023-01-22T09:24:12.127243Z",
          "shell.execute_reply.started": "2023-01-22T09:24:12.124241Z"
        },
        "id": "gMx33Dpk-C14"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T03:22:18.914497Z",
          "iopub.status.busy": "2023-01-22T03:22:18.914214Z",
          "iopub.status.idle": "2023-01-22T04:05:44.917022Z",
          "shell.execute_reply": "2023-01-22T04:05:44.916399Z",
          "shell.execute_reply.started": "2023-01-22T03:22:18.914480Z"
        },
        "id": "rQyJVPhe-C15",
        "outputId": "7e284081-46be-4426-f2f9-d2b5c5fe7d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 4.3315, val loss 4.3307\n",
            "step 500: train loss 1.5400, val loss 1.7231\n",
            "step 1000: train loss 1.2676, val loss 1.5193\n",
            "step 1500: train loss 1.1402, val loss 1.4896\n",
            "step 2000: train loss 1.0315, val loss 1.5094\n",
            "step 2500: train loss 0.9176, val loss 1.5621\n",
            "step 3000: train loss 0.8020, val loss 1.6332\n",
            "step 3500: train loss 0.6810, val loss 1.7247\n",
            "step 4000: train loss 0.5660, val loss 1.8492\n",
            "step 4500: train loss 0.4599, val loss 1.9535\n",
            "step 4999: train loss 0.3708, val loss 2.0861\n",
            "\n",
            "How young lamb'd about with a rest wife a shamed breach\n",
            "For being a many more of good tower two youung;\n",
            "Which was it in that growth the base do good,\n",
            "To comfort to give the state, which they conceal the story.\n",
            "\n",
            "Boy:\n",
            "But, for I go that our order died delight,\n",
            "I am concerning by the faith were manh\n",
            "I am lacker to death mo well and husband,\n",
            "As felt that I have in justice which I chance.\n",
            "But fear, so will I.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Good danger, no man word till the world with words.\n",
            "\n",
            "GLOUCESTER:\n",
            "Clarence told \n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters -1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:05:44.917996Z",
          "iopub.status.busy": "2023-01-22T04:05:44.917797Z",
          "iopub.status.idle": "2023-01-22T04:06:04.609791Z",
          "shell.execute_reply": "2023-01-22T04:06:04.608925Z",
          "shell.execute_reply.started": "2023-01-22T04:05:44.917970Z"
        },
        "id": "hk4UFzRu-C15",
        "outputId": "1c0b1337-5d1d-4c29-da6f-3b0702ad54c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RICHARD:\n",
            "Nay, if the help-will orator all himself,\n",
            "Will close not hold on him. but, let him speak,\n",
            "Let him calm point. Advery well too.\n",
            "So, farewell, my well; good aunt.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "I pray thee, good Montague may the compay.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Why faith be so, it is the matter, sure so.\n",
            "\n",
            "RICHARD:\n",
            "I come, I crack to pardon me, if I cannot learn.\n",
            "\n",
            "DORSET:\n",
            "The purple breath of thine own: seal out, now prove awond:\n",
            "Whats's the best will comfort, to the name that\n",
            "Which God's good for itself, that have woe\n",
            "The mighty power ever! O, what very\n",
            "Minus death have weed them hewish him,\n",
            "Or her my fance? O, find his kingdom both with speed;\n",
            "One of the wind which time histoes disparage\n",
            "Of dignity, a viperod, shed with a signorant\n",
            "In merciful time: her comes who comes by his\n",
            "false against herewith children of recreant,\n",
            "execution was where he is: I may with him well.\n",
            "The warrant of your fornications afford\n",
            "And the best fix'd by Angelo in banishment,\n",
            "Who strives in his light: this house with patience\n",
            "Lie down with doing, and false bar with the pettictor.\n",
            "\n",
            "ESCALUS:\n",
            "Say you? go along with her! we'll anger to:\n",
            "thou wilt came to be, I shall at once.\n",
            "\n",
            "Duke MENENIUS:\n",
            "Nay, where have you?\n",
            "\n",
            "First, Marcius:\n",
            "I thought thee here, but that you'ld that be done,\n",
            "command with a punusuado.\n",
            "\n",
            "Second Murderer:\n",
            "O, I beseech you, help to my words!\n",
            "\n",
            "CORIOLANUS:\n",
            "It is a fawn that duares now best to do\n",
            "You will throw all the Roman: the ship side,\n",
            "I thought the Capulet:--O, why, then I thank you,\n",
            "If you know me, we shall better custom off the heart\n",
            "Than my honour to give it: worst if in those whom?\n",
            "It will most venturel with mine oes\n",
            "That thou shalt her comfort; both Bolingbroke\n",
            "In horse, you changether grace. You have at learn\n",
            "As I can deal unproved no other horrily,\n",
            "So hate ended the senators. Besides, and you\n",
            "Blend the grace of monstery, and that your\n",
            "executioner, and franklow no sacrifice.\n",
            "\n",
            "First Gentleman:\n",
            "Who would find, were I\n",
            "ontended o' the pedlar; I shall practise to him.\n",
            "And to these knaves, I'll go with hope of care\n",
            "God save holy and sentence is foolistering else.\n",
            "\n",
            "Second Gentleman:\n",
            "I thought that makes these covertures wars.\n",
            "\n",
            "Third Gentleman:\n",
            "Welcome, good my nail.\n",
            "\n",
            "LUCIO:\n",
            "But, the wack that I have reason to see it?\n",
            "\n",
            "Third Messenger:\n",
            "Then very creepets would be at quarrel day,\n",
            "So doing with France in jads with suffering horse.\n",
            "\n",
            "Servant:\n",
            "What companion I should lay a hareless to stead\n",
            "on him, and my petty revenge!\n",
            "\n",
            "DORCAULIO:\n",
            "A gloomy favour!\n",
            "\n",
            "LEONTES:\n",
            "Very noon, but by the fair debt in base.\n",
            "Go, see thy sorright; all some disht is mine own.\n",
            "\n",
            "FLORIZEL:\n",
            "Then mercy those that was never with thee\n",
            "That thou be guilty ever graced with him.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "What woe is that, who hath praised these brokes\n",
            "And many a deceit duty in him?\n",
            "It will then such from me that truth love\n",
            "That not the king cozened by the fault of love:\n",
            "Cannstrup bread for and due I meant her?\n",
            "These the devil, thou shalt thou still have; Clare.\n",
            "Dost thou not holdship to hell die?\n",
            "For I refuse thy fortunes for my poison,\n",
            "And do break the power of thine outward fain:\n",
            "For I'll mine enemy, forfeit with thee,\n",
            "Shame home for my order as the dead law:\n",
            "Didst thou not here, bid teach me No more than\n",
            "Thou hatts find that shore:\n",
            "These battlache where thou art the list\n",
            "In piercipable of soon wounded shapes do that\n",
            "In whole of him; but, one for those would meet,\n",
            "One whose name could have change,--thot render percise\n",
            "About this wreak again to be hot.\n",
            "\n",
            "CORIOLANUS:\n",
            "Tush!\n",
            "Those good days are within, good metimes,\n",
            "Loak off to the volume and world's dangerous,\n",
            "Scratching the stock that moraling the firm,\n",
            "I like my body's face; life had betray'd\n",
            "Furthenceforthwarn me for a lord's death.\n",
            "Not sleep my soul, for burthen, for I doit it.\n",
            "\n",
            "FRIAR SCAPULET:\n",
            "I'll make thee shapes for thee of myself:\n",
            "Revenge me, comfort; am I come to him.\n",
            "More foolish than any oath: no face many fade;\n",
            "We may promise the nothing but shall stea.\n",
            "\n",
            "ROMEO:\n",
            "They still will be; let on hast those joys thine.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Holy Sir Perus begives me mine are pastime to wear.\n",
            "\n",
            "LADY GREY:\n",
            "I fear, as thou lies without of thy tears:\n",
            "Thou hadst keep'd each off the time of thee,\n",
            "For thou hast not stirring toward thy coat,\n",
            "Thou hadst ne'er seen thy brother dear me.\n",
            "\n",
            "KING RICHARD III:Wt then?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "The comfort hath miss it won.\n",
            "\n",
            "KING RICHARD II:\n",
            "And bid to Ravel:\n",
            "Say that I will sing have; I have deserved thee\n",
            "As many of the woe, honour more.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Grandam, let the mistress have been thou tear it.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Good my soul, I say.\n",
            "\n",
            "RIVERS:\n",
            "Then shall be mine own against those which I shall\n",
            "Once detect the wanton slumper of a fear.\n",
            "I, that am not I found to fear;\n",
            "I burn by absence boat a place many suits,\n",
            "To sign a band down with new bait, or bestow\n",
            "Caeal'd, and all noble to bed wife\n",
            "In satisfied age.\n",
            "\n",
            "Gaoler:\n",
            "How hast thou the matter?\n",
            "\n",
            "SAMPSON:\n",
            "I saw the worse give I have, banish'd him;\n",
            "Being at home to some other were his bastard borne\n",
            "In lamentation than my love!\n",
            "\n",
            "Second Murderer:\n",
            "What's Caius is ch\n"
          ]
        }
      ],
      "source": [
        "print(decode(model.generate(context, max_new_tokens=5000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:09:59.034071Z",
          "iopub.status.busy": "2023-01-22T04:09:59.033388Z",
          "iopub.status.idle": "2023-01-22T04:09:59.037194Z",
          "shell.execute_reply": "2023-01-22T04:09:59.036507Z",
          "shell.execute_reply.started": "2023-01-22T04:09:59.034047Z"
        },
        "id": "II5tSkZD-C15"
      },
      "outputs": [],
      "source": [
        "# data_x = []\n",
        "# for i in range(10):\n",
        "#     for k in range(10):\n",
        "#         for k in range(10):\n",
        "#             \"\".join(str(i), str(j), str(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-22T04:06:04.979263Z",
          "iopub.status.idle": "2023-01-22T04:06:04.979476Z",
          "shell.execute_reply": "2023-01-22T04:06:04.979384Z",
          "shell.execute_reply.started": "2023-01-22T04:06:04.979372Z"
        },
        "id": "ExBT3M0j-C15"
      },
      "outputs": [],
      "source": [
        "if 'text' in globals():\n",
        "    print(\"yeah\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTZ0TjKz-C15"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:09:36.615007Z",
          "iopub.status.busy": "2023-01-22T04:09:36.614374Z",
          "iopub.status.idle": "2023-01-22T04:09:36.631128Z",
          "shell.execute_reply": "2023-01-22T04:09:36.630628Z",
          "shell.execute_reply.started": "2023-01-22T04:09:36.614983Z"
        },
        "id": "oC1CTvEL-C16"
      },
      "outputs": [],
      "source": [
        "list_text = [\"story.txt\"]\n",
        "\n",
        "with open(list_text[0], 'r', encoding='utf-8') as f:\n",
        "    text_all = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:09:38.989730Z",
          "iopub.status.busy": "2023-01-22T04:09:38.989085Z",
          "iopub.status.idle": "2023-01-22T04:09:38.994357Z",
          "shell.execute_reply": "2023-01-22T04:09:38.993921Z",
          "shell.execute_reply.started": "2023-01-22T04:09:38.989705Z"
        },
        "id": "QWJN7SF6-C16",
        "outputId": "f030bfa8-c84e-46a7-babb-e03f593e49c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f282a40c470>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 512\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:10:53.240364Z",
          "iopub.status.busy": "2023-01-22T04:10:53.240077Z",
          "iopub.status.idle": "2023-01-22T04:10:53.253840Z",
          "shell.execute_reply": "2023-01-22T04:10:53.253146Z",
          "shell.execute_reply.started": "2023-01-22T04:10:53.240343Z"
        },
        "id": "2DKvUl2l-C16",
        "outputId": "956e59fb-86b1-48b5-a3e3-97388bad3a53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "157"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = sorted(list(set(text_all)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:10:57.505469Z",
          "iopub.status.busy": "2023-01-22T04:10:57.505183Z",
          "iopub.status.idle": "2023-01-22T04:10:57.642290Z",
          "shell.execute_reply": "2023-01-22T04:10:57.641596Z",
          "shell.execute_reply.started": "2023-01-22T04:10:57.505447Z"
        },
        "id": "6vwB4bHs-C16",
        "outputId": "bbc371a7-26e0-46c0-a160-d68984588c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "19.198109 M parameters\n"
          ]
        }
      ],
      "source": [
        "print(device)\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "# m = torch.compile(mod01)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:11:02.920971Z",
          "iopub.status.busy": "2023-01-22T04:11:02.920682Z",
          "iopub.status.idle": "2023-01-22T04:11:02.995200Z",
          "shell.execute_reply": "2023-01-22T04:11:02.994383Z",
          "shell.execute_reply.started": "2023-01-22T04:11:02.920950Z"
        },
        "id": "1DrQLYqp-C16",
        "outputId": "5ed8d4b4-7faa-49e5-cb34-2abaea2180cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "850142\n",
            "94461\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text_all), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(len(train_data))\n",
        "print(len(val_data))\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data)- block_size, (batch_size,))\n",
        "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:11:05.965307Z",
          "iopub.status.busy": "2023-01-22T04:11:05.965015Z",
          "iopub.status.idle": "2023-01-22T04:11:05.969509Z",
          "shell.execute_reply": "2023-01-22T04:11:05.968782Z",
          "shell.execute_reply.started": "2023-01-22T04:11:05.965285Z"
        },
        "id": "9edH5MNG-C17"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:11:08.641579Z",
          "iopub.status.busy": "2023-01-22T04:11:08.641293Z",
          "iopub.status.idle": "2023-01-22T04:54:38.529650Z",
          "shell.execute_reply": "2023-01-22T04:54:38.528994Z",
          "shell.execute_reply.started": "2023-01-22T04:11:08.641558Z"
        },
        "id": "peXD0Biv-C17",
        "outputId": "ec3007d4-bd77-45cc-99c0-7c24c66dec52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 5.2351, val loss 5.2232\n",
            "step 500: train loss 1.5566, val loss 1.9454\n",
            "step 1000: train loss 1.2250, val loss 1.6973\n",
            "step 1500: train loss 1.0462, val loss 1.6849\n",
            "step 2000: train loss 0.8900, val loss 1.7581\n",
            "step 2500: train loss 0.7353, val loss 1.8757\n",
            "step 3000: train loss 0.5880, val loss 2.0398\n",
            "step 3500: train loss 0.4559, val loss 2.2587\n",
            "step 4000: train loss 0.3448, val loss 2.4505\n",
            "step 4500: train loss 0.2666, val loss 2.5799\n",
            "step 4999: train loss 0.2096, val loss 2.7382\n",
            "\n",
            "[FN#241] Entertain gro and necklace cause shall hardly with such talk\n",
            "favour of him.\" During then he used to another serve understored and\n",
            "the Enverthing with drink at the guard and pinioned, the Mount of\n",
            "Tiberian noblers and the Jinn boiled about doeth for us risk and\n",
            "escates more unless than the sleep. And she bent a suffice whose Thousand\n",
            "Nights and when she waxed, \"Wilt direct me a crier instruct of stuff\n",
            "of my poor deeds, rived in a sole or tatrip of force and trusty cast intent thin\n",
            "thy ha\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters -1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:54:38.530987Z",
          "iopub.status.busy": "2023-01-22T04:54:38.530778Z",
          "iopub.status.idle": "2023-01-22T04:54:46.812773Z",
          "shell.execute_reply": "2023-01-22T04:54:46.812168Z",
          "shell.execute_reply.started": "2023-01-22T04:54:38.530969Z"
        },
        "id": "FU_FzFym-C17",
        "outputId": "c95423bf-ac24-4ff5-a4ff-21ab9d8d5b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[FN#140] for the Porter Mahik of Pleasur. Moslemh = the tribe, for\n",
            "which Destiny have seen how the infinite frea of hashes ever design.\n",
            "\n",
            "\n",
            "The rade oddrably as Nubian and is also Subh (a little showed line), and\n",
            "profusion was to show whose Arabic Mahammed's King King of the Banu who\n",
            "bade beating me; and the two bitches being graced with their Kings person\n",
            "and his offixed weeping only a procuratrix as the three hunch took a piece.\"\n",
            "\n",
            "As we entreated the bird Nubian [FN#3] in which this world-was) is castled with\n",
            "soon from which shall net have occasion the save country of the\n",
            "work from the devire of me. The young man shot himself to come,\n",
            "on one day he remain such an oath.\" I translated to him and, take thy son of\n",
            "thine and even man and thou shalt say! So Allah well ask?—what have bewelcome\n",
            "of this man and the moon whom hath usurely cur!by Allah he slained him for this\n",
            "very condition and wot how tasteful; how thou hadst honoured of my men\n",
            "for thee and what hath befallen thee for thy desire.\" Quoth\n",
            "the Ifrit, \"For,\" and he, \"no must Shams al-Din, \"to there less\n",
            "and this is a cap which hath not befallen me, the matter of the sons of\n",
            "Adam, the ill-aowant to smile from the MaChúkam,[FN#537] by the Caliph\n",
            "of white slaves take and the old man be. However I went forth\n",
            "from it and came upondered the kindon wherein was the knew thousand\n",
            "dirhams. Then I took himself the reign and cast to do so meet her down\n",
            "and instruction. Presently sundocided the cabitan in hight, and I lain\n",
            "guardens of forty damsels to die. So I sent for my husband and\n",
            "shaking and joined with it full morth; but his country was breed. So I\n",
            "rejoiced with exceeding joy at my eyes, and summoned my fancy where he\n",
            "was at once and was also as thou desiresolate in which the reader\n",
            "knows not white. All the water is bewilder force trouble than way of\n",
            "the ragout, [FN#9ching the tale of the soutOry of Allah Most Highest Most\n",
            "High ; even as a the poet shitting by something black of the sell, and the\n",
            "people may the will kno\n"
          ]
        }
      ],
      "source": [
        "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T04:54:46.813800Z",
          "iopub.status.busy": "2023-01-22T04:54:46.813580Z",
          "iopub.status.idle": "2023-01-22T04:54:47.616228Z",
          "shell.execute_reply": "2023-01-22T04:54:47.615322Z",
          "shell.execute_reply.started": "2023-01-22T04:54:46.813780Z"
        },
        "id": "5bo_5cHi-C18",
        "outputId": "5c3b4e89-ae86-4d24-ed1f-9dbf1543a426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jan 22 04:54:47 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA RTX A4000    Off  | 00000000:00:05.0 Off |                  Off |\n",
            "| 66%   81C    P2   103W / 140W |   9155MiB / 16376MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLWJmeNX-C18"
      },
      "source": [
        "- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?\n",
        "\n",
        "\n",
        "For this, I am going to use the model in above that just trained on \"One Thousand and One Nights\" to use it as a pretraining to train the Shakespeare dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAyl3oQj-C18"
      },
      "source": [
        "For this part, I am guessing it is asking me to do Transformer Learning.\n",
        "\n",
        "\n",
        "For EX3, it asked us to train on the larger dataset and train the Shakespeare after, this\n",
        "is a transfer learning. If so, we need to freeze the trained weight and remove the head. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:29:35.119937Z",
          "iopub.status.busy": "2023-01-22T09:29:35.119340Z",
          "iopub.status.idle": "2023-01-22T09:29:35.137163Z",
          "shell.execute_reply": "2023-01-22T09:29:35.136403Z",
          "shell.execute_reply.started": "2023-01-22T09:29:35.119909Z"
        },
        "id": "BdG18FSx-C18",
        "outputId": "c7ca7ba9-c2b4-4103-e08f-f37a24f00b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:29:37.031608Z",
          "iopub.status.busy": "2023-01-22T09:29:37.031320Z",
          "iopub.status.idle": "2023-01-22T09:29:37.046428Z",
          "shell.execute_reply": "2023-01-22T09:29:37.045885Z",
          "shell.execute_reply.started": "2023-01-22T09:29:37.031587Z"
        },
        "id": "JQtGjiL--C18"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:29:38.816432Z",
          "iopub.status.busy": "2023-01-22T09:29:38.815602Z",
          "iopub.status.idle": "2023-01-22T09:29:38.912102Z",
          "shell.execute_reply": "2023-01-22T09:29:38.911472Z",
          "shell.execute_reply.started": "2023-01-22T09:29:38.816408Z"
        },
        "id": "dAaZQJ1M-C18"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data)- block_size, (batch_size,))\n",
        "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKww-_39-C19"
      },
      "source": [
        "Save the model for safety reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T05:07:22.233399Z",
          "iopub.status.busy": "2023-01-22T05:07:22.232602Z",
          "iopub.status.idle": "2023-01-22T05:07:37.081165Z",
          "shell.execute_reply": "2023-01-22T05:07:37.080593Z",
          "shell.execute_reply.started": "2023-01-22T05:07:22.233374Z"
        },
        "id": "2YpqSovb-C19"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), 'oton1.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:28:54.414780Z",
          "iopub.status.busy": "2023-01-22T09:28:54.414492Z",
          "iopub.status.idle": "2023-01-22T09:28:54.748171Z",
          "shell.execute_reply": "2023-01-22T09:28:54.747337Z",
          "shell.execute_reply.started": "2023-01-22T09:28:54.414759Z"
        },
        "id": "9DaW3HGy-C19"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), 'init_pre1.pt')\n",
        "model.lm_head = nn.Linear(n_embd, 157)\n",
        "model.load_state_dict(torch.load('oton1.pt'))\n",
        "model.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:30:22.270917Z",
          "iopub.status.busy": "2023-01-22T09:30:22.270367Z",
          "iopub.status.idle": "2023-01-22T09:30:22.274911Z",
          "shell.execute_reply": "2023-01-22T09:30:22.274366Z",
          "shell.execute_reply.started": "2023-01-22T09:30:22.270895Z"
        },
        "id": "kSD6zlxr-C19",
        "outputId": "fe7864a8-cbeb-4065-f5da-53320c443cb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T06:45:37.162683Z",
          "iopub.status.busy": "2023-01-22T06:45:37.161989Z",
          "iopub.status.idle": "2023-01-22T06:45:37.169344Z",
          "shell.execute_reply": "2023-01-22T06:45:37.168790Z",
          "shell.execute_reply.started": "2023-01-22T06:45:37.162656Z"
        },
        "id": "FbO3WNUQ-C19",
        "outputId": "dcd7b665-4f45-4306-c27b-0c35fdd79d2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.0000e-05, 1.0186e-05, 1.0376e-05, 1.0569e-05, 1.0766e-05, 1.0967e-05,\n",
              "        1.1171e-05, 1.1379e-05, 1.1591e-05, 1.1807e-05])"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lre = torch.linspace(-5, -1, 500)\n",
        "lrs = 10**lre\n",
        "\n",
        "\n",
        "lri = []\n",
        "lossi = []\n",
        "stepi = []\n",
        "lrs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T06:45:42.251481Z",
          "iopub.status.busy": "2023-01-22T06:45:42.251184Z",
          "iopub.status.idle": "2023-01-22T06:45:42.262601Z",
          "shell.execute_reply": "2023-01-22T06:45:42.261990Z",
          "shell.execute_reply.started": "2023-01-22T06:45:42.251460Z"
        },
        "id": "YQqlQNEb-C19",
        "outputId": "db91e57c-3952-4241-ecca-18f61562be82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19.150913 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = model.to(device)\n",
        "# m = torch.compile(mod01)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T06:45:43.904780Z",
          "iopub.status.busy": "2023-01-22T06:45:43.904479Z",
          "iopub.status.idle": "2023-01-22T06:49:55.661515Z",
          "shell.execute_reply": "2023-01-22T06:49:55.660819Z",
          "shell.execute_reply.started": "2023-01-22T06:45:43.904757Z"
        },
        "id": "Xzg-lOWk-C19",
        "outputId": "1b664e00-274d-4c53-b001-1078ea6ed9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 4.2822, val loss 4.2927\n"
          ]
        }
      ],
      "source": [
        "for iter in range(500):\n",
        "    if iter % eval_interval == 0 or iter == max_iters -1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    \n",
        "    lr = lrs[iter]\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    lri.append(lre[iter])\n",
        "    stepi.append(iter)\n",
        "    lossi.append(loss.log10().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T06:33:05.092749Z",
          "iopub.status.busy": "2023-01-22T06:33:05.092188Z",
          "iopub.status.idle": "2023-01-22T06:33:06.088779Z",
          "shell.execute_reply": "2023-01-22T06:33:06.088123Z",
          "shell.execute_reply.started": "2023-01-22T06:33:05.092728Z"
        },
        "id": "ei7GN6jG-C19"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T06:49:55.662762Z",
          "iopub.status.busy": "2023-01-22T06:49:55.662571Z",
          "iopub.status.idle": "2023-01-22T06:49:55.745747Z",
          "shell.execute_reply": "2023-01-22T06:49:55.745168Z",
          "shell.execute_reply.started": "2023-01-22T06:49:55.662744Z"
        },
        "id": "xqxFUe4i-C1-",
        "outputId": "c4aed502-37c2-4e5b-840c-c30197b93768"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f275c42c790>]"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(lri, lossi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T08:25:12.175144Z",
          "iopub.status.busy": "2023-01-22T08:25:12.174564Z",
          "iopub.status.idle": "2023-01-22T08:25:12.477925Z",
          "shell.execute_reply": "2023-01-22T08:25:12.477212Z",
          "shell.execute_reply.started": "2023-01-22T08:25:12.175122Z"
        },
        "id": "NsJwFB3i-C1-"
      },
      "outputs": [],
      "source": [
        "model.lm_head = nn.Linear(n_embd, 157)\n",
        "model.load_state_dict(torch.load('oton1.pt'))\n",
        "model.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T08:25:16.224358Z",
          "iopub.status.busy": "2023-01-22T08:25:16.224075Z",
          "iopub.status.idle": "2023-01-22T09:08:21.662188Z",
          "shell.execute_reply": "2023-01-22T09:08:21.661497Z",
          "shell.execute_reply.started": "2023-01-22T08:25:16.224338Z"
        },
        "id": "Z4Ze8vZf-C1-",
        "outputId": "885e67dc-849a-4647-d7f1-2ae42429e11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 4.4042, val loss 4.4045\n",
            "step 500: train loss 1.2204, val loss 1.5078\n",
            "step 1000: train loss 1.0597, val loss 1.5050\n",
            "step 1500: train loss 0.9807, val loss 1.4968\n",
            "step 2000: train loss 0.9441, val loss 1.5107\n",
            "step 2500: train loss 0.9149, val loss 1.5319\n",
            "step 3000: train loss 0.8847, val loss 1.5432\n",
            "step 3500: train loss 0.8565, val loss 1.5605\n",
            "step 4000: train loss 0.8292, val loss 1.5735\n",
            "step 4500: train loss 0.8036, val loss 1.6006\n",
            "step 4999: train loss 0.7768, val loss 1.6116\n",
            "\n",
            "Forberth, mocking that they say true Rutland's true of tears,\n",
            "Witness many story be hereafter:\n",
            "Therefore they are all they that stand quick links:\n",
            "Yield they of the midst chequeme gravity Rome\n",
            "at least. My name is Banelingbroke?\n",
            "\n",
            "Nay, fetch!\n",
            "\n",
            "Nurse:\n",
            "Will you shall he know how the Troy and respect?\n",
            "Uncle, foolish to Mantua: the people hither life\n",
            "Before those high and have writ. Prithee, think not the\n",
            "strict stumble-proof. I honourably minca's bosom,\n",
            "save a very relering and honour'd. Thou liest,\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters -1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    \n",
        "    if(iter == 1000):\n",
        "        for g in optimizer.param_groups:\n",
        "            g['lr'] = 8e-5\n",
        "    \n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:08:21.663923Z",
          "iopub.status.busy": "2023-01-22T09:08:21.663450Z",
          "iopub.status.idle": "2023-01-22T09:08:42.750067Z",
          "shell.execute_reply": "2023-01-22T09:08:42.749390Z",
          "shell.execute_reply.started": "2023-01-22T09:08:21.663900Z"
        },
        "id": "5rHbVSXu-C1-",
        "outputId": "15f45d8f-d1c2-40e6-d6fb-65cbc1df58e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Instigatingale.\n",
            "\n",
            "LEONTES:\n",
            "Ay, there's a lessernness,\n",
            "That I will return before a footment's eyeborn,\n",
            "Good fortune Hereford, and far stick his armour,\n",
            "And laugh upon bands and good loving souls;\n",
            "On pity of death-bloody, Warwick, let's hell.\n",
            "\n",
            "LORD ROSS:\n",
            "Pheris, Vaughan, she might be the crow'd in part.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Lord Edward, cohse that me were from our brothers?\n",
            "\n",
            "KING RICHARD III:\n",
            "Ay the king of fresh Dion, and this fault that give.\n",
            "My partuous sons, God, keep the hold of majesty,\n",
            "The king the Duke of Norfolk life, but kiss\n",
            "Time enmity: we shall go call King Henry's grows\n",
            "Aiming Lewis of hers, for the best. Wife, nobless and tremble,\n",
            "Even the infectiven thousand Guilus,\n",
            "Kind newly did behind; and 'twere the gentleman\n",
            "When the mapest way, they are further;\n",
            "Enship to come the cares, Clarence cannot\n",
            "Leave in my antitude to resign fear.\n",
            "But I tell you,--for this sight before it haze\n",
            "The true house of my pancient reply,\n",
            "Without you book of your hands: it is not,\n",
            "To make but one noish and harlot. Answer my:\n",
            "But none are care you are gone?\n",
            "\n",
            "ANGELO:\n",
            "And, with like a scratch and content to yover:\n",
            "\n",
            "Provost:\n",
            "I shall say for your will, if you'll keep:\n",
            "More words is that, man; indegly, remember,\n",
            "That, I am casque five young by, like alone.\n",
            "There is your love, the duke of York.\n",
            "\n",
            "GLOUCESTER:\n",
            "Your enemies, and let them ask you.\n",
            "\n",
            "LADY ANNE:\n",
            "So the seas of ENr-Geoply I so house,\n",
            "As our subjects deceived my hate were all.\n",
            "Second Lord George and Dorsets are gone.\n",
            "\n",
            "MISTRESS OVERDONE:\n",
            "O methinks I here sleep indeed!\n",
            "\n",
            "LADY ANNE:\n",
            "I wish your highness, mind here flies for their prodigy;\n",
            "And near they gave him out.\n",
            "\n",
            "POLIXENES:\n",
            "This is somen soen, that's a greater cheat,\n",
            "We seem a mistress on us foe,\n",
            "And full on coming with dispute, all touch'd fears.\n",
            "This little souls cook of minds ere may discharge\n",
            "Their carest what I could this dispatch'd without\n",
            "The glies of all her fast. The jealous my Rome,\n",
            "The mutinous pale hook behind; I will not so;\n",
            "Shall a true for outcor's, as I fear.\n",
            "I bless him to stand fast, since there.\n",
            "Are merried of mouths? or opinious enemy\n",
            "Puts and prayers of his own days to-night!\n",
            "\n",
            "PARIS:\n",
            "Ay; I pray the lead, my husband.\n",
            "\n",
            "PARIS:\n",
            "Why took you to the city issue?\n",
            "\n",
            "JULIET:\n",
            "We cannot blow thy face, I'll send it liquor;\n",
            "Enquoth it: you were ado me for parliament. Puddle\n",
            "Have had her sport: and therefore you both, sire my\n",
            "songlect make an entrative to make a due alcient,\n",
            "unlay'd upon smile, and seize the chamber'd Rutland known;\n",
            "My grands look'd with a vow of this treacherous war,\n",
            "Turn his cripp'd ears to my rapier part,\n",
            "And is adopted peparation or stale;\n",
            "Our smiling kiss doubtful water of war.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "My loyalty till take your sullen pronounces\n",
            "To fly that were a poor of meat, drawn to seek-eye.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Your grace consently cousin!\n",
            "\n",
            "KING RICHARD II:\n",
            "Look, cousin, fair judgment! cause my cheek\n",
            "Let's plant to still, yet shall so ground allish.\n",
            "Now, sir's too cloudy, the noise of your title,\n",
            "Should heaven in the breath o'ercaling arms,\n",
            "Could split them to close him? Calais, boy, his\n",
            "subtlesses! Why, the duke, is past friends, heart\n",
            "sometimely storm; and swake his years with him.\n",
            "O, boy, mark! that good my father; here\n",
            "Gloverst reputation of the poor marriage speed\n",
            "in Hermion; he shall be hehnd.\n",
            "\n",
            "Shepherd:\n",
            "He shall be.\n",
            "\n",
            "La, son:\n",
            "Know, she hath: if he be king his that our hate,\n",
            "'That like a half, that wilt thou have ask'd:\n",
            "The manly one high prejoicies, which he is no harm,\n",
            "And being in whats, is strange, ere I was guided\n",
            "And granted to my forenood?' Be but 't so good.'\n",
            "\n",
            "GLOUCESTER:\n",
            "But seek the quench of else, brother, you not.\n",
            "\n",
            "BISHOP OF CARLISC, belike.\n",
            "\n",
            "CAMILLO:\n",
            "What is that I may wish?\n",
            "\n",
            "GLOUCESTER:\n",
            "Why?\n",
            "\n",
            "BALTHASAR:\n",
            "I pray not thee, I think welcome.\n",
            "What is my honesty thousand? my lord will I rite.\n",
            "\n",
            "KING HENRY VI:\n",
            "Right, and how I should not pronounce myself\n",
            "A monarch will yet say 'fore again.\n",
            "Read of Edward, take not you all!\n",
            "\n",
            "GLBT:\n",
            "How say'st thou not?\n",
            "\n",
            "CATESBY:\n",
            "Thing me, my lord,\n",
            "Whom I thought by my masterTase I can,\n",
            "But no broaches that, which answer'd me who,\n",
            "We to furthee I'll make it thereof;\n",
            "For we pass'd in this silent wave for a sleep.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Are then?\n",
            "\n",
            "HASTINGS:\n",
            "If any will speak down in battle; and\n",
            "Repen in bransming heat and fear against thy clouds.\n",
            "\n",
            "GLOUCESTER:\n",
            "Now, Clarence traitor: there how then persuaded signify,\n",
            "Shalt my evillant be whole and made outpation,\n",
            "And fearly stinging thee, will attend thee in respect\n",
            "Of witnession the ommongs, boy: I thought,\n",
            "See thou to't, in the eye yea on earth to give\n",
            "Awake my leave torn my hide their wife farewer:\n",
            "And let me kill thy chammer them serve the fled.\n",
            "One father! what might not they must\n",
            "Be blown his promise than they have and not\n",
            "Uphop of many fortune, to marquest him in;\n",
            "And there timeless be stranger'd the crown, now,\n",
            "Than two forth of humiliar me a\n",
            "hand and rosemary him.\n",
            "\n",
            "POLIXENES:\n",
            "Good grace with us, nobles abread!\n",
            "Romeo is neard the less of war, Romeo?\n",
            "Shall I guess thanks that plucks your enemies?\n",
            "O, is it within the gr\n"
          ]
        }
      ],
      "source": [
        "print(decode(model.generate(context, max_new_tokens=5000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:08:42.753421Z",
          "iopub.status.busy": "2023-01-22T09:08:42.753095Z",
          "iopub.status.idle": "2023-01-22T09:08:44.176568Z",
          "shell.execute_reply": "2023-01-22T09:08:44.175905Z",
          "shell.execute_reply.started": "2023-01-22T09:08:42.753397Z"
        },
        "id": "yak-6Nwk-C1-"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), 'final_result.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:08:44.177621Z",
          "iopub.status.busy": "2023-01-22T09:08:44.177415Z",
          "iopub.status.idle": "2023-01-22T09:09:13.740547Z",
          "shell.execute_reply": "2023-01-22T09:09:13.739926Z",
          "shell.execute_reply.started": "2023-01-22T09:08:44.177601Z"
        },
        "id": "EoesKOg0-C1-",
        "outputId": "fb870262-98c1-4824-d430-75aff05d9f74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Grame:\n",
            "Yet stands to abistre: but thou hast, sure thy land stir\n",
            "it will fly too: find thy songs of thy face,\n",
            "And fall batterwise budted toutment;\n",
            "Srean thou, thy wife and thyself in thy bed;\n",
            "Death, dog, mispate, and yet mull all tongue,\n",
            "MEARDANE:\n",
            "Be Earl of Warwick, I would yet have my comfort.\n",
            "But what loss what title?  noble Mars!\n",
            "The say is King of coming spectacle\n",
            "Enjust the waves. Where have we now the wars\n",
            "My mercy wrong to be cut of his own child,\n",
            "Servant Clarence and for Duke of York,\n",
            "Northame this dukes is changed royal by\n",
            "The noble incle Richmond of the Lord Hastings.\n",
            "To-morrow all to hear him to take his bed:\n",
            "Advice cousin, wisdom from our foes!\n",
            "\n",
            "KING RICHARD II:\n",
            "Norfolk, thou hade out no further love,\n",
            "Shalt post in bereft us prison: equest, given,\n",
            "Doing what end our choppier to any power\n",
            "Than keep, to keep a back'd for holding:\n",
            "But since thou canst not break no farther stop,\n",
            "Tell Richard, sound a calls and love-sleepy,\n",
            "That thou stand'st thus music and stold, their\n",
            "beggar's like time, and fortune me well: 'tis a\n",
            "gerlaim to make the walls\n",
            "From that they. I am services that lass I might stay.\n",
            "\n",
            "LORD FORD:\n",
            "Lords, you take my true knowld comes.\n",
            "\n",
            "LORD ROSS:\n",
            "Thou ever many uncled Rivers, well not laugh cold;\n",
            "But such art ten times, he walked as far\n",
            "Is beastlically tongued: therefore had dream'd\n",
            "To this fair disobs of mine enemy\n",
            "Hearinested and kingdom and unking, mighted in me,\n",
            "In blacks of me and the son, boy,\n",
            "Gives mine own, and make the like a germy\n",
            "Of those fellowships: with thousand crossing cliens,\n",
            "That thou peep'st was to the hollow-beare's queen;\n",
            "But thou sinest so doth my dear tirest go.\n",
            "\n",
            "YORK:\n",
            "Ay, because thy faith, present fled heaven;\n",
            "Be calm up your contented with him that made\n",
            "More than the sweets all, old ends, majesty\n",
            "Is not him right for thee to look upon.\n",
            "I had this torgue me more, nor not thy hour:\n",
            "But where comes in her counsel the grief names,\n",
            "Not to give Hereford's stard sound himself,\n",
            "That sent left thy sister's ring herew's down.\n",
            "\n",
            "Lord:\n",
            "Whose valiant for Richmond's widow? what! lords he?\n",
            "\n",
            "First Lord:\n",
            "Stay him will come; he will stand aloof,\n",
            "And with his large hand; for he, here has time\n",
            "He thought and we brought our strengths upon:\n",
            "Look is weak to't, behold them here.\n",
            "Hoise our city's arms is now to mighty;\n",
            "Made feantipoint in some seven well\n",
            "The soften of mostimentable whipp'd.\n",
            "Boy, Hortension was my father's hated wrath\n",
            "To want the biron\n",
            "And what owe more you awhile stands woon:\n",
            "The state have at the golden so which hideen\n",
            "Scratching the minners: so that hath calr'd\n",
            "In hope is false; a gentleman and true,\n",
            "Test in the desire of my queen's virtue;\n",
            "Long close them gaze in the house; I should\n",
            "Shall feely not the king's son, and on partly poison\n",
            "And made her requite of my sweet brother:\n",
            "When how broaches hath wenty suggest thou have,\n",
            "And known to behind a starp upon the Lady\n",
            "Make the walls allow'd upon: and thou kill'st,\n",
            "Thou hast minister'd, they must be lough'd.\n",
            "\n",
            "BAGHOOR:\n",
            "Peace, peace! how from this forth of thy love?\n",
            "\n",
            "BALTHASAR:\n",
            "Then, to Pomfret, I hold. Then, Tranio!\n",
            "\n",
            "BALTHASAR:\n",
            "Biondee, lords, for I have heard your knows;\n",
            "Your flue shall after ourself: good aunt,\n",
            "Relate, in that you might, presently, noble Marcius,\n",
            "Your noble uncle, not pity 'Menenius,\n",
            "Your queen image i' the desert bleming pardon,\n",
            "So supremite settled aside; and the influtts flight\n",
            "Against the treacherous world, there is proudes\n",
            "the from of the purse.\n",
            "\n",
            "ESCALUS:\n",
            "On, the furthuous Dorse, I would say.\n",
            "Now thou dost thyself a horse; there dies,\n",
            "It lies plent in those thieves, thy own silence's,\n",
            "Whosevereus were that compass in wooer'd\n",
            "Will take the naked, unfort: one of thee,\n",
            "And more than thou let's name, the rest man--\n",
            "Sinnerd, that's seen the north. To-morrow, than to\n",
            "curse ceatures him upon mine army peace.\n",
            "\n",
            "SICINIUS:\n",
            "I mean to do the heart of many that thou\n",
            "wouldst the nose of their honours.\n",
            "\n",
            "Below:\n",
            "In that there's nor being peers thee tilts.\n",
            "\n",
            "Second Messenger:\n",
            "A sair, noble lord, how be the watch'd!\n",
            "We were bequall'd thee, and I had lived you me:\n",
            "You kill'd I am a king and will as be\n",
            "Remainted to run a pardon; for I am off\n",
            "Thou camest-follow, i' violent, come hither: though I\n",
            "cannot tender how I met your gaoler, men with you\n",
            "sit might be plucked fived. What must I am!\n",
            "Mast be done a month: and would I should, make my chest\n",
            "To that shame him? And hath I may hanged a\n",
            "match'd too my friend most goodman, when I\n",
            "Wild make mine own again any way soon,\n",
            "Nothing on my bread and hath bearer'd in descent.\n",
            "\n",
            "GLOUCESTER:\n",
            "Well, poor men, when I weep we see my sister.\n",
            "Bidst them more thy flattery: let my girl;\n",
            "And there eat again present my gage, my\n",
            "sorrow to be there hew follows: goods be now to\n",
            "bearing the mother, and bear them miss excellent night.\n",
            "\n",
            "Third Citizen:\n",
            "And so we devour! how for our kindred's pright,\n",
            "That lies explainly well along, I slipp'd?\n",
            "\n",
            "Second 'er:\n",
            "I'll tell thee husband from wedding, and lent her.\n",
            "\n",
            "Second Citizen:\n",
            "All son of him!' Consis  gentleman:\n",
            "'Tis from me: an honesty' name of truth.'\n",
            "\n",
            "Third Citizen:\n",
            "'Faith, it shall be sometime of it: it sharp no\n",
            "son, the's the way the face of his earlots to our\n",
            "fowns in all neighbours.\n",
            "\n",
            "SICINIUS:\n",
            "Go then, being most strange: 'hou buy\n",
            "a person, gentlemen, and you have had kind myself\n",
            "much only ready for the realm.\n",
            "\n",
            "AUFIDIUS:\n",
            "Imp it south him, indeed ignorant,\n",
            "Quoe that, is the market-place on each, since office any malice\n",
            "on's pawn.\n",
            "\n",
            "a peeceper:\n",
            "I warrant thee take by me to limit, and in a\n",
            "more upon cure of us otempt and lick of you.\n",
            "\n",
            "POLIXENES:\n",
            "Prithee, ben, rat, my good father, be so:\n",
            "Thou shalt not be executed into us; yet\n",
            "meant my soul, to answer me for troop: if not\n",
            "bail struckles news than this boy the fire coals\n",
            "That speaks nothing but thence. Friar Lord F\n",
            "Fresching to thy house, or I know not what: if I\n",
            "Hope the house of this rude wrong, they should woo\n",
            "right by oath with ripen'd bones, would I had no such\n",
            "charity.\n",
            "\n",
            "Shepherd:\n",
            "But, master.\n",
            "\n",
            "POLIXENES:\n",
            "We'll marry.\n",
            "\n",
            "CAMILLO:\n",
            "I do bear a little may since be consistinging,\n",
            "Nor newly make me joy.\n",
            "\n",
            "ABHORSON:\n",
            "Sir, who good sir?\n",
            "\n",
            "GLOUCESTER:\n",
            "Let Mannortal watch upon your own,\n",
            "To make a limm interpretty and hearts\n",
            "Thou over-mark'd aside their estimage,\n",
            "Making their within themselves: therefore,\n",
            "pray not with Francience of the sanctuary,\n",
            "Two have cush'd thereof.\n",
            "\n",
            "GLOUCESTER:\n",
            "O, let's play their treasury\n",
            "Where we mistake, and my cause far into the Tower,\n",
            "Where; for well, know, craves me amen!\n",
            "I'll obey of youngest, because you be now,\n",
            "Why let you must away this castle,\n",
            "Not you to this house that caught him thence,\n",
            "By he; who faces but nine imprisonment's trust\n",
            "Bed his knowledge and waility be holp'd.\n",
            "\n",
            "KING HELEWIS XI:\n",
            "Ah, Warwick, king! what shame you fairly kings?\n",
            "If ever were done? O plague blows,\n",
            "This it remember wounded is down\n",
            "That war't revenge on the garter of smiles,\n",
            "Such detemples of children's interruption\n",
            "Had by child with his broad;\n",
            "Since it was a general, which part of guilt to be?\n",
            "And favour the law of love and written in\n",
            "bore 'being that which I did.\n",
            "\n",
            "LEONTES\n"
          ]
        }
      ],
      "source": [
        "print(decode(model.generate(context, max_new_tokens=7000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:48:28.518091Z",
          "iopub.status.busy": "2023-01-22T09:48:28.517281Z",
          "iopub.status.idle": "2023-01-22T09:48:28.572300Z",
          "shell.execute_reply": "2023-01-22T09:48:28.571786Z",
          "shell.execute_reply.started": "2023-01-22T09:48:28.518065Z"
        },
        "id": "_5DwVhm1-C1-",
        "outputId": "1dcd7fef-cd5b-47de-88f5-581141236760"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.save(model.state_dict(), 'init_pre1.pt')\n",
        "model.lm_head = nn.Linear(n_embd, 65)\n",
        "# vocab_size = 157\n",
        "model.load_state_dict(torch.load('final_result.pt'))\n",
        "# model.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:49:28.891494Z",
          "iopub.status.busy": "2023-01-22T09:49:28.890671Z",
          "iopub.status.idle": "2023-01-22T09:49:28.961729Z",
          "shell.execute_reply": "2023-01-22T09:49:28.961233Z",
          "shell.execute_reply.started": "2023-01-22T09:49:28.891468Z"
        },
        "id": "sWAgMTiY-C1_",
        "outputId": "ad2b9c68-cc85-4ab1-e46b-63cee88817f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_text = [\"story.txt\"]\n",
        "\n",
        "with open(\"input.txt\", 'r', encoding='utf-8') as f:\n",
        "    text_all = f.read()\n",
        "chars = sorted(list(set(text_all)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:56:09.655024Z",
          "iopub.status.busy": "2023-01-22T09:56:09.654164Z",
          "iopub.status.idle": "2023-01-22T09:56:11.808321Z",
          "shell.execute_reply": "2023-01-22T09:56:11.807660Z",
          "shell.execute_reply.started": "2023-01-22T09:56:09.654996Z"
        },
        "id": "mPazzVsG-C1_",
        "outputId": "e1133b4b-8a8f-41a0-9258-b51e47ccb787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apple: tree\n",
            "Green: yes\n",
            "Jump: down\n",
            "\n",
            "q apple is cool green\n",
            "a tree is cool yes\n",
            "\n",
            "q Jump green random\n",
            "and can rotten, citizens upon his love;\n",
            "And she bark her; for within me ne'er I did.\n",
            "Now, if we mine affect thyself excuses\n",
            "Of ten over eyes for the house of Hermione.\n",
            "\n",
            "DUKE OF YORK:\n",
            "If then?\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "No, no; he is just.\n",
            "\n",
            "KING RICHARD III:\n",
            "Thanks, Vmagood duke: sometime, thou canst myself\n",
            "Put upon Thursday; which way our noble company\n",
            "These deof judges the purier of the mire\n",
            "With them all greets. Pita, they shall find too:\n",
            "The papering leave it for the deep,--but it stinted\n",
            "with oath and\n"
          ]
        }
      ],
      "source": [
        "question1 = \"Apple: tree\\nGreen: yes\\nJump: down\\n\\nq apple is cool green\\na tree is cool yes\\n\\nq Jump green random\\na\"\n",
        "q1 = torch.tensor(encode(question1)).unsqueeze(0)\n",
        "model = model.to(device)\n",
        "q1 = q1.long().to(device)\n",
        "print(decode(model.generate( q1, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:55:14.041686Z",
          "iopub.status.busy": "2023-01-22T09:55:14.041044Z",
          "iopub.status.idle": "2023-01-22T09:55:14.046271Z",
          "shell.execute_reply": "2023-01-22T09:55:14.045726Z",
          "shell.execute_reply.started": "2023-01-22T09:55:14.041661Z"
        },
        "id": "N4k0hy0J-C1_",
        "outputId": "73942aeb-32e3-4284-a64e-3021c052894a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[13, 54, 54, 50, 43, 10,  1, 58, 56, 43, 43,  0, 19, 56, 43, 43, 52, 10,\n",
              "          1, 63, 43, 57,  0, 22, 59, 51, 54, 10,  1, 42, 53, 61, 52,  0,  0, 55,\n",
              "          1, 39, 54, 54, 50, 43,  1, 47, 57,  1, 41, 53, 53, 50,  1, 45, 56, 43,\n",
              "         43, 52,  0, 39,  1, 58, 56, 43, 43,  1, 47, 57,  1, 41, 53, 53, 50,  1,\n",
              "         63, 43, 57,  0,  0, 55, 22, 59, 51, 54,  1, 45, 56, 43, 43, 52,  1, 56,\n",
              "         39, 52, 42, 53, 51,  0, 39]])"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question1 = \"Apple: tree\\nGreen: yes\\nJump: down\\n\\nq apple is cool green\\na tree is cool yes\\n\\nqJump green random\\na\"\n",
        "q1 = torch.tensor(encode(question1)).unsqueeze(0)\n",
        "q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:43:41.436109Z",
          "iopub.status.busy": "2023-01-22T09:43:41.435311Z",
          "iopub.status.idle": "2023-01-22T09:43:41.445366Z",
          "shell.execute_reply": "2023-01-22T09:43:41.444805Z",
          "shell.execute_reply.started": "2023-01-22T09:43:41.436084Z"
        },
        "id": "ns3Vl5M_-C1_",
        "outputId": "b42ec253-2def-48c6-8116-31f91fb2cb84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1])"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "context.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-22T09:49:58.531582Z",
          "iopub.status.busy": "2023-01-22T09:49:58.531002Z",
          "iopub.status.idle": "2023-01-22T09:49:58.954642Z",
          "shell.execute_reply": "2023-01-22T09:49:58.954092Z",
          "shell.execute_reply.started": "2023-01-22T09:49:58.531556Z"
        },
        "id": "JULkVl5C-C1_",
        "outputId": "fc15d456-94cb-4b30-8b57-413a9b396664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "And, after belind about forth:\n",
            "But, upon thy food day I here will shed:\n",
            "About join instantly crange \n"
          ]
        }
      ],
      "source": [
        "model = model.to(device)\n",
        "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}